{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AczRCzbE5HG",
        "outputId": "fa8af77d-5008-4c50-f3d2-307f2d5bb019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to output/log.txt\n",
            "\n",
            "PyTorch version: 2.2.1+cu121\n",
            "CUDA version: 12.1\n",
            "\n",
            "Random Seed:  1\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /root/Data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4403733.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /root/Data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /root/Data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 66238.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /root/Data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /root/Data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1247870.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/Data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /root/Data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4426238.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/Data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/Data/mnist/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 [0/118] loss_D_real: 1.1193 loss_D_fake: 1.0719 loss_G: 0.6338\n",
            "Epoch 0 [100/118] loss_D_real: 0.0419 loss_D_fake: 2.5201 loss_G: 0.0941\n",
            "Epoch 1 [0/118] loss_D_real: 0.0375 loss_D_fake: 2.4327 loss_G: 0.1002\n",
            "Epoch 1 [100/118] loss_D_real: 0.0759 loss_D_fake: 1.2287 loss_G: 0.3565\n",
            "Epoch 2 [0/118] loss_D_real: 0.0961 loss_D_fake: 0.8784 loss_G: 0.5415\n",
            "Epoch 2 [100/118] loss_D_real: 0.2251 loss_D_fake: 0.2097 loss_G: 1.7283\n",
            "Epoch 3 [0/118] loss_D_real: 0.1145 loss_D_fake: 0.9495 loss_G: 0.5048\n",
            "Epoch 3 [100/118] loss_D_real: 0.2387 loss_D_fake: 0.9187 loss_G: 0.5259\n",
            "Epoch 4 [0/118] loss_D_real: 0.1300 loss_D_fake: 1.0635 loss_G: 0.4441\n",
            "Epoch 4 [100/118] loss_D_real: 0.1349 loss_D_fake: 1.4154 loss_G: 0.2956\n",
            "Epoch 5 [0/118] loss_D_real: 0.1523 loss_D_fake: 0.8614 loss_G: 0.5650\n",
            "Epoch 5 [100/118] loss_D_real: 0.2526 loss_D_fake: 0.6003 loss_G: 0.8260\n",
            "Epoch 6 [0/118] loss_D_real: 0.2247 loss_D_fake: 0.6952 loss_G: 0.7192\n",
            "Epoch 6 [100/118] loss_D_real: 0.1086 loss_D_fake: 1.6503 loss_G: 0.2287\n",
            "Epoch 7 [0/118] loss_D_real: 0.2107 loss_D_fake: 0.7853 loss_G: 0.6310\n",
            "Epoch 7 [100/118] loss_D_real: 0.3000 loss_D_fake: 0.6746 loss_G: 0.7377\n",
            "Epoch 8 [0/118] loss_D_real: 0.2673 loss_D_fake: 0.7159 loss_G: 0.7020\n",
            "Epoch 8 [100/118] loss_D_real: 0.2168 loss_D_fake: 1.0135 loss_G: 0.4694\n",
            "Epoch 9 [0/118] loss_D_real: 0.3377 loss_D_fake: 0.6710 loss_G: 0.7425\n",
            "Epoch 9 [100/118] loss_D_real: 0.2481 loss_D_fake: 0.7283 loss_G: 0.6834\n",
            "Epoch 10 [0/118] loss_D_real: 0.0789 loss_D_fake: 1.6834 loss_G: 0.2208\n",
            "Epoch 10 [100/118] loss_D_real: 0.1379 loss_D_fake: 1.2599 loss_G: 0.3482\n",
            "Epoch 11 [0/118] loss_D_real: 0.1805 loss_D_fake: 0.9593 loss_G: 0.5028\n",
            "Epoch 11 [100/118] loss_D_real: 0.1341 loss_D_fake: 1.0995 loss_G: 0.4234\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vmy_utils\n",
        "\n",
        "import my_utils\n",
        "\n",
        "\n",
        "CUDA = True     # Change to False for CPU training\n",
        "DATA_PATH = '~/Data/mnist'\n",
        "#DATA_PATH = '/media/john/FastData/CelebA'\n",
        "# DATA_PATH = '/media/john/FastData/lsun'\n",
        "OUT_PATH = 'output'\n",
        "LOG_FILE = os.path.join(OUT_PATH, 'log.txt')\n",
        "BATCH_SIZE = 512        # Adjust this value according to your GPU memory\n",
        "IMAGE_CHANNEL = 1\n",
        "# IMAGE_CHANNEL = 3\n",
        "Z_DIM = 100\n",
        "G_HIDDEN = 64\n",
        "X_DIM = 64\n",
        "D_HIDDEN = 64\n",
        "EPOCH_NUM = 12\n",
        "REAL_LABEL = 1\n",
        "FAKE_LABEL = 0\n",
        "lr = 2e-4\n",
        "seed = 1            # Change to None to get different results at each run\n",
        "\n",
        "my_utils.clear_folder(OUT_PATH)\n",
        "print(\"Logging to {}\\n\".format(LOG_FILE))\n",
        "sys.stdout = my_utils.StdOut(LOG_FILE)\n",
        "CUDA = CUDA and torch.cuda.is_available()\n",
        "print(\"PyTorch version: {}\".format(torch.__version__))\n",
        "if CUDA:\n",
        "    print(\"CUDA version: {}\\n\".format(torch.version.cuda))\n",
        "\n",
        "if seed is None:\n",
        "    seed = np.random.randint(1, 10000)\n",
        "print(\"Random Seed: \", seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if CUDA:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "cudnn.benchmark = True      # May train faster but cost more memory\n",
        "\n",
        "dataset = dset.MNIST(root=DATA_PATH, download=True,\n",
        "                      transform=transforms.Compose([\n",
        "                      transforms.Resize(X_DIM),\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.5,), (0.5,))\n",
        "                      ]))\n",
        "#dataset = dset.ImageFolder(root=DATA_PATH,\n",
        "#                           transform=transforms.Compose([\n",
        "#                           transforms.Resize(X_DIM),\n",
        "#                           transforms.CenterCrop(X_DIM),\n",
        "#                           transforms.ToTensor(),\n",
        "#                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "#                           ]))\n",
        "# dataset = dset.LSUN(root=DATA_PATH, classes=['bedroom_train'],\n",
        "#                     transform=transforms.Compose([\n",
        "#                     transforms.Resize(X_DIM),\n",
        "#                     transforms.CenterCrop(X_DIM),\n",
        "#                     transforms.ToTensor(),\n",
        "#                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "#                     ]))\n",
        "\n",
        "assert dataset\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    \"\"\"custom weights initialization\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # 1st layer\n",
        "            nn.ConvTranspose2d(Z_DIM, G_HIDDEN * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(G_HIDDEN * 8),\n",
        "            nn.ReLU(True),\n",
        "            # 2nd layer\n",
        "            nn.ConvTranspose2d(G_HIDDEN * 8, G_HIDDEN * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(G_HIDDEN * 4),\n",
        "            nn.ReLU(True),\n",
        "            # 3rd layer\n",
        "            nn.ConvTranspose2d(G_HIDDEN * 4, G_HIDDEN * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(G_HIDDEN * 2),\n",
        "            nn.ReLU(True),\n",
        "            # 4th layer\n",
        "            nn.ConvTranspose2d(G_HIDDEN * 2, G_HIDDEN, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(G_HIDDEN),\n",
        "            nn.ReLU(True),\n",
        "            # output layer\n",
        "            nn.ConvTranspose2d(G_HIDDEN, IMAGE_CHANNEL, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # 1st layer\n",
        "            nn.Conv2d(IMAGE_CHANNEL, D_HIDDEN, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 2nd layer\n",
        "            nn.Conv2d(D_HIDDEN, D_HIDDEN * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(D_HIDDEN * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 3rd layer\n",
        "            nn.Conv2d(D_HIDDEN * 2, D_HIDDEN * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(D_HIDDEN * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # 4th layer\n",
        "            nn.Conv2d(D_HIDDEN * 4, D_HIDDEN * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(D_HIDDEN * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # output layer\n",
        "            nn.Conv2d(D_HIDDEN * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, 1).squeeze(1)\n",
        "\n",
        "\n",
        "netG = Generator().to(device)\n",
        "netG.apply(weights_init)\n",
        "print(netG)\n",
        "\n",
        "netD = Discriminator().to(device)\n",
        "netD.apply(weights_init)\n",
        "print(netD)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "viz_noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1, device=device)\n",
        "\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "for epoch in range(EPOCH_NUM):\n",
        "    for i, data in enumerate(dataloader):\n",
        "        x_real = data[0].to(device)\n",
        "\n",
        "        real_label = torch.full((x_real.size(0),), REAL_LABEL, device=device,dtype=torch.float32)\n",
        "        fake_label = torch.full((x_real.size(0),), FAKE_LABEL, device=device,dtype=torch.float32)\n",
        "\n",
        "        # Update D with real data\n",
        "        netD.zero_grad()\n",
        "        output_real = netD(x_real)\n",
        "        loss_D_real = criterion(output_real, real_label)\n",
        "        loss_D_real.backward()\n",
        "        D_x = output_real.mean().item()\n",
        "\n",
        "\n",
        "\n",
        "        # Update D with fake data\n",
        "        z_noise = torch.randn(x_real.size(0), Z_DIM, 1, 1, device=device)\n",
        "        x_fake = netG(z_noise)\n",
        "        output_fake = netD(x_fake.detach())  # Detach to prevent gradients from flowing into G\n",
        "        loss_D_fake = criterion(output_fake, fake_label)\n",
        "        loss_D_fake.backward()\n",
        "        D_G_z1 = output_fake.mean().item()\n",
        "        # optimizerD.step()\n",
        "\n",
        "        # Update G with fake data\n",
        "        netG.zero_grad()\n",
        "        output_fake = netD(x_fake)\n",
        "        loss_G = criterion(output_fake, real_label)\n",
        "        loss_G.backward()\n",
        "        D_G_z2 = output_fake.mean().item()\n",
        "\n",
        "        # Update the discriminator and generator\n",
        "        optimizerD.step()\n",
        "        optimizerG.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('Epoch {} [{}/{}] loss_D_real: {:.4f} loss_D_fake: {:.4f} loss_G: {:.4f}'.format(\n",
        "                epoch, i, len(dataloader),\n",
        "                loss_D_real.mean().item(),\n",
        "                loss_D_fake.mean().item(),\n",
        "                loss_G.mean().item()\n",
        "            ))\n",
        "            vmy_utils.save_image(x_real, os.path.join(OUT_PATH, 'real_samples{}.png'.format(epoch)), normalize=True)\n",
        "            with torch.no_grad():\n",
        "                viz_sample = netG(viz_noise)\n",
        "                vmy_utils.save_image(viz_sample, os.path.join(OUT_PATH, 'fake_samples_{}.png'.format(epoch)), normalize=True)\n",
        "    torch.save(netG.state_dict(), os.path.join(OUT_PATH, 'netG_{}.pth'.format(epoch)))\n",
        "    torch.save(netD.state_dict(), os.path.join(OUT_PATH, 'netD_{}.pth'.format(epoch)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XZqhDNbKW8y",
        "outputId": "3b894bd5-46b8-4fce-de9d-8df78e8fb138"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([96, 1, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "data[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQ3nsOHRPuMS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}